\chapter{Experiment}\label{ch:experiment}

This chapter describes the experiment conducted in simulation and on a robotic system. First, section \cref{sec:simulated_scenario} describes the simulated scenario in use to carry out the experiments in simulation and on a robotic system. Then, the specifications of the experimental setups are described in \cref{sec:experiment_setup}. After that, the evaluation framework used to analyze the experimental results is presented in \cref{sec:evaluation_framework}. Finally, the analysis of the experimental results is presented in \cref{sec:analysis}.

% --------------------------------------------------------
\section{Simulated scenario}\label{sec:simulated_scenario}
% --------------------------------------------------------

To run experiments both in simulation and on real robots, this thesis implements a simulated scenario of a factory warehouse with one \gls{amr} and obstacles. The simulation is recorded and replayed for each experiment run to reduce the computation overhead of the simulation and to make the experiment repeatable. Furthermore, in order to compare the results from the simulation and the real robot, the simulation recording is used as input for both experiments under the same replay settings. 

\subsection{Environment and AMR simulation}

% include a bird-view shot for the simulation with warehouse, robot, and human obstacles in the view.
The scenario simulates a modern-day factory warehouse, illustrated in \cref{fig:bird_view_scenario}. The simulated scenario is implemented in \gls{gazebo} with the help of a software package called "scenario execution", which the author implemented during his work at \gls{amsrl}. The scenario consists of a factory warehouse environment and static obstacles, such as humans, boxes, shelves, and pallets, which are common obstacles in a factory warehouse. Since the performance difference among different \gls{yolo}v5 models are negligible when the scene is too simple, this thesis includes ten human obstacles in the simulated scenario to increase the complexity of the object detection task. Moreover, some human obstacles are occluded by other obstacles in the scenario. Detecting occluded objects has been proven to be a challenging task in object detection \cite{Saleh2021}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/sim/bird_view.png}
    \caption{Bird view of the simulated scenario}
    \label{fig:bird_view_scenario}
\end{figure}

The \gls{amr} is implemented as a dynamic obstacle in the scenario and it moves along a pre-defined path. The pre-defined path is made up of a series of waypoints in the simulation. Once the \gls{amr} reaches the current waypoint, the scenario execution will assign the next waypoint to the \gls{amr}. After all waypoints are successfully reached, the scenario execution shuts down the simulation. In the simulation, the \gls{amr} does not collide with other obstacles. The \gls{amr} is equipped with an RGB camera that is located on top of the robot and facing forward. The intrinsic parameters of the RGB camera sensor are listed in \cref{tab:camera_params}. During the simulation, the camera can observe all of the obstacles in the simulation with partial occlusion of some obstacles. In addition to the RGB image stream, the simulated camera sensor also outputs an image stream that contains the ground truth for the bounding boxes in the object detection task using the bounding box sensor from \gls{gazebo}. The ground truth data and the image stream are bridged from the \gls{gazebo} simulation topics to \gls{ros} topics so that the offloading pipeline can make use of them. In \cref{fig:robot_view_scenario}, it shows from the point of view of the \gls{amr}'s camera sensors. On the left, it shows the illustration of the bounding box ground truth data, while the image stream is shown on the right. The bounding box camera sensor and the RGB camera sensor are positioned the same and intend to simulate an actual point of view of a normal \gls{amr}.

\begin{table}[htp]
    \centering
    \begin{tabular}{lc}
    \toprule
    Parameter&                  Value\\
    \midrule
    Height&                     848 pixels\\
    Width&                      480 pixels\\
    FOV&                        1.047\\
    Frame rate&                 30 frames/sec\\
    Noise type&                 Gaussian\\
    Noise mean&                 0\\
    Noise standard deviation&   0.007\\
    \bottomrule
    \end{tabular}
    \caption{RGB camera intrinsic parameters}
    \label{tab:camera_params}
\end{table}

% describes how the robot is navigated, describes how the camera is mounted on the robot

\begin{figure}
    \centering
    \begin{subfigure}[h]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sim/rgb_img.png}
        \caption{RGB image}
        \label{fig:robot_view_scenario:rgb_image}
    \end{subfigure}
    \begin{subfigure}[h]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sim/bbgt.png}
        \caption{Bounding box ground truth}
        \label{fig:robot_view_scenario:bbgt}
    \end{subfigure}
    \caption{Robot view of the simulated scenario}
    \label{fig:robot_view_scenario}
\end{figure}

% Move to limitations
% It is worth pointing out that no dynamic obstacles are implemented in the simulated scenario. However, the \gls{amr} is dynamic and the camera sensors are mounted on top of it. Therefore, in the viewpoint of the \gls{amr}, the obstacles are constantly moving and thus could be treated as dynamic solely for the object detection task. Another reason for this is that the fresh out-of-box bounding box sensor from \gls{gazebo} is inaccurate for dynamic obstacles like humans because dynamic human actors are implemented as the actors in \gls{gazebo} and the animation of the actors is realized by the deformation of the meshes of the actor model. Unfortunately, the bounding box sensor can only provide the bounding boxes of the undeformed actor. Therefore, the bounding boxes available are not accurate to serve as the ground truth for the object detection task. Till the finish of the thesis, the problem is not resolved with the current version of \gls{gazebo} this thesis uses. However, static human obstacles and dynamic \gls{amr} still constitute an adequate simulated scenario for the research questions this thesis is trying to address. 

\subsection{Record and Replay}

% describes how the ROS bag is recorded and how is it replayed during the experiments and explain why this is needed. 
The simulation takes up a great amount of the resources of computers. To reduce the overhead of running the simulation alongside the experiments, this thesis records the simulation to a \gls{ros} bag and replays it during the experiment runs. This reduces the \gls{cpu} and memory usage of the host machine during the experiment and improves the performance of the rest of the offloading pipeline. As mentioned in \cref{sec:frameworks_and_tools}, \gls{ros} provides the functionality to record the topics and allows the users to replay them later while preserving the same publishing rate and order of the messages. More importantly, this thesis conduct experiment on a real robotic system with limited resources. Such a system is not capable of running real-time simulations while maintaining the rest of the offloading framework. In real-world applications, \glspl{amr} only needs to maintain the driver of the camera and minimal software to be able to get the same image input. Therefore, in \cref{tab:ros_bag_comparison}, this thesis presents an experiment comparing the \gls{cpu} usage of replaying a \gls{ros} bag and running the library for the Intel RealSense camera. The results show that the two processes have comparable \gls{cpu} usage on a robotic system. Furthermore, \gls{gazebo} slows down the simulation time compared to real-time when the system is strained. Therefore, to ensure a real-time simulation, specifying a replay rate of the \gls{ros} bag can guarantee the simulation time factor is within a reasonable range. 

\begin{table}[htp]
    \centering
    \begin{tabular}{lc}
    \toprule
    Process&                    CPU usage\\
    \midrule
    \gls{ros} bag replay&       14.12\%\\
    RealSense camera library&   11.28\%\\
    \bottomrule
    \end{tabular}
    \caption{Comparison of CPU usage on robotic system}
    \label{tab:ros_bag_comparison}
\end{table}

All topics during the simulation are recorded in the \gls{ros} bag, including the RGB images, the bounding box ground truth, the camera information, etc. The image stream consists of 1748 frames and the \gls{ros} bag is replayed at around 25 frames per second during the experiment. Topics for RGB image stream and the bounding box ground truth are replayed with best effort reliability \gls{qos} setting to ensure the RGB images are replayed the same way it is recorded. Since the \gls{ros} topics are recorded at a different time than the experiment, the time stamps of the RGB image messages and the ground truth messages have to be overwritten by the receiving time of the offloading node.

% ---------------------------------------------------------------
\section{Experimental Setup}\label{sec:experiment_setup}
% ---------------------------------------------------------------

In this section, the experimental setups for experiments in simulation and on a robotic system are specified. The baseline offloading strategies are first evaluated in simulation experiments to test their functionality and robustness. Then, the baseline strategies as well as the dynamic strategy are evaluated on an actual robotic system. 

\subsection{Simulation experiment setup}

For experiments in simulation, the host machine is equipped with an Intel(R) Core(TM) i9-7900X \gls{cpu} and an Nvidia GeForce GTX 1060 6GB \gls{gpu}. The \gls{cpu} has a total of 20 cores. In order to simulate the \gls{amr}'s and the edge computer in realistic conditions, this thesis limits the computation resources of the \gls{docker} containers. The robot container is constrained to using only eight cores of the host machine, while the edge container is given ten cores. In addition, the edge computer has access to the \gls{gpu} of the host machine and calculates the image inference on the \gls{gpu}. Moreover, the robot container is also constrained to using only 4 GB of memory, while the memory of the edge container is not constrained and the host machine has access to 32 GB of memory, including the 4 GB memory assigned to the robot container.

The limitations of CPU and memory usage are realized by tools provided by \gls{docker} itself. According to the \gls{docker} documentation, the runtime memory limitation is realized by Memory Resource Controller provided by the \gls{linux} kernel. Furthermore, the \gls{docker} container can enforce hard memory limitation and soft memory limitation at runtime. The former does not allow the container to use any amount more than specified, while the latter only kicks in when certain conditions are met. In this thesis, we use the hard memory limitation on the robot container to simulate the hardware constraints. For CPU usage, the limitation is realized by the CFS scheduler, which is the \gls{linux} kernel CPU scheduler for normal \gls{linux} processes. This indicates that the limitation on CPU usage is achieved by limiting the accessible CPU cycles of the \gls{docker} container. The GPU of the host machine is exposed to the \gls{docker} containers using the Nvidia Container Toolkit.

In addition to resource limitation simulation, the network is also simulated to achieve realistic conditions for the robot and edge containers. The containers use the default \gls{docker} bridge network. Since the simulated scenario \gls{ros} bag is replayed in the robot container and the output data are also recorded in the robot container, the data flow between the robot container and the edge container only consists of offloaded images and detection processed by the edge container. To allow the data flow of around 25 frames per second of images with a resolution of 848 x 640 pixels, which is around 40 MB per second, the packet limit of in and out policies of the \gls{netem} is set to 20000. The in and out delays are set to 50 ms for both policies. To investigate the influence of the network bandwidth, the experiments are carried out under two bandwidth conditions: with a 160 Mbps bandwidth constraint and no bandwidth constraints at all. The former aims to represent a limited network bandwidth that is unable to handle the data flow that is needed for a full offloaded execution, while the latter represents an unlimited network bandwidth. 

For \gls{qos} settings, the offloading module uses a "best effort" reliable policy to prevent the publisher from being blocked by the bag network connection caused by the bandwidth constraints. As mentioned in \cref{sec:frameworks_and_tools}, only \gls{fast_dds} allows a true asynchronous publishing mode. Therefore, Fast-DDS is used for \gls{ros} middleware, and the publishing mode is set to asynchronous for the experiments. The offloading module uses the "keep last" queuing settings and the queue size is set to 5. Furthermore, the \gls{ros} bag replay of the simulated scenario and the state monitors also use "best effort" reliable policy settings to simulate a camera sensor. 

\subsection{Robot experiment setup}

Unlike the aforementioned simulation experiments, the robot experiment uses two separate computers for the \gls{amr}'s onboard system and the edge computer. For the robot, the onboard system uses a \gls{nuc} equipped with an Intel(R) Core(TM) i3-8109U \gls{cpu}, which is commonly used for \glspl{amr}.  For the edge, a \gls{linux} desktop computer equipped with an Intel(R) Core(TM) i3-8109U \gls{cpu} and an Nvidia GeForce GTX 1060 6GB \gls{gpu} is used. On both computers, the offloading module and the perception modules have full access to the available resources. The \gls{ros} bag replay of the simulated scenario is carried out on the robot. The output data of the metrics are also recorded on the robot. The edge computer is solely responsible for the inference of the offloaded images and monitoring the edge system states and sending them back to the robot. The image inference is computed by the \gls{gpu} on the edge computer with the help of \gls{pytorch} library, while the image inference on the robot is computed by its onboard \gls{cpu}. As mentioned in \cref{sec:state_monitors}, the system states of the robot and the edge are measured with the "system status" package.

Two network interfaces are used to conduct the robot experiments. First, the experiment uses an Ethernet connection between the two computers. Similar to the simulation experiments, two network conditions are tested under the Ethernet connection. First, an Ethernet network interface without bandwidth constraints is used to simulate a perfect network connection between the robot and the edge. Then, the Ethernet network interface is constrained to 160 Mbps bandwidth to simulate a constrained network connection. Finally, a \gls{wifi} network interface is used to carry out the experiment. The \gls{wifi} network has unstable network bandwidth from time to time and therefore affects the experiment results drastically. This is to investigate different offloading strategies under dynamic network condition changes. When conducting the experiment with one network interface, the other one is shut down to prevent the offloading framework from using both network interfaces simultaneously. 

Similar to the simulation experiment, \gls{fast_dds} is used as middleware for \gls{ros}. The \gls{qos} reliability policy for offloading is set to "best effort" and the publishing mode of \gls{fast_dds} is set to asynchronous to prevent the publisher from being blocked by the network. Moreover, the queuing settings use the "keep last" policy and the queue size is set to five messages. Under various \gls{qos} settings, this setup achieves the best performance of the object detection task. However, other \gls{qos} settings are also investigated during the experiments. In \cref{ch:appendix_a}, the results of the experiment with a "reliable" reliability policy are presented. 

% ------------------------------------------------------------
\section{Evaluation Framework}\label{sec:evaluation_framework}
% ------------------------------------------------------------

In order to understand the influence of different offloading strategies on the robot, an evaluation framework is introduced. More specifically, this section describes the metrics which this thesis uses to evaluate the results of the experiments and discusses the evaluation methods used to process the data. 

\subsection{Metrics}
% This section lists all the evaluation metrics used and how they are recorded and what tools are used in order to record them. 
 

The evaluation metrics used by this thesis fall into two categories. The first category focuses mainly on the resources of the \gls{amr}. This includes CPU usage, power consumption, and bandwidth usage. These metrics represent the onboard resources for computation, energy, and network used by the \gls{amr}. For these metrics, this thesis also subtracts the results when the offloading pipeline is idling as a baseline to eliminate the influence of processes other than the perception modules on the metrics. More specifically, while idling, the offloading framework still launches the offloading node and the perception nodes. It also replays and records the ROS bags. However, the offloading module is not sending any messages to the onboard perception module or the edge perception module, i.e., there are no object detection tasks being performed at all. 

The second category describes the performance of the object detection task, including average precision, execution latency, and the overall processed frame rate. For the average precision, only the human obstacles are considered in the evaluation. The detection from the perception nodes is compared with the bounding box ground truth from the simulation. The evaluation algorithm is provided by the package "torchauto" as a part of the code base from \gls{amsrl}.  For the execution latency, the two components, i.e., the network delay and the inference time, are evaluated separately for offloading and local computation. In addition, to evaluate how the offloading framework is performing over the entire simulation, the overall processed frame rate evaluates how many frames of all frames are processed by either perception module.

\subsection{Evaluation methods}

For the comparison between the detection and the ground truth, the time stamps of the messages are crucial to the final result. This raises the question: what should be considered as simultaneity for the detection and the ground truth in task offloading? This thesis presents two evaluation methods of the \gls{ap} metric in order to gain some insights into this question. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figures/setup/sync_eval.pdf}
    \caption{Synchronous evaluation}
    \label{fig:sync_eval}
\end{figure}

In the synchronous evaluation, the detection is evaluated by being compared with the ground truth of the same time stamp. As illustrated in \cref{fig:sync_eval}, the image is either processed by the robot perception module or the edge perception module. The processed result and the ground truth are recorded by the ROS bag. After the experiment runs, the recorded result and the ground truth with the nearest time stamps are grouped as one data frame by the message filter. The message filter groups the messages from different ROS topics. The proximate messages within a time threshold can be considered as the same data frame. The threshold is currently set to 0.05 seconds. The is calculated within the same data frame. For the entire experiment run, the mean and the standard deviation are calculated for all the data frames. This means the data frame is discarded if no detection or ground truth is present for the specific time stamp, i.e., the \gls{ap} will not be affected if the robot onboard system or the edge computer is not capable of processing the image. The capability of processing the image is however reflected in the metric of the overall processed frame percentage. 

The synchronous evaluation method does not take the execution latency into consideration, because the detection is compared against the ground truth with the same time stamp as the original image. Therefore, the final result interpolates the \glspl{ap} of the edge computer and the robot onboard system. However, this also does not represent the offloading scenario accurately. In real-world applications, the \glspl{amr} move continuously. The current situation of the \gls{amr} could be very different than the moment the image is taken by the camera if the processing takes too long and the detection is outdated. Therefore, the execution latency plays an important role in the accuracy of how well the \glspl{amr} perceives the environment. 

With the aforementioned considerations, this thesis investigates an asynchronous evaluation method that takes the execution latency into consideration, which is the evaluation method for \gls{ap} metric that the thesis adopts. As illustrated in \cref{fig:async_eval}, the detection is compared against the images with the time stamps when the \glspl{amr} actually receive the processed result. This is achieved by changing the time stamps of the detection messages to the receiving time of the \glspl{amr}. Depending on how much the images have changed, the quality of the detection deteriorates with the execution latency, which is usually the case in real-world applications.

In the evaluation framework of this thesis, since the pair bound between the ground truth and the detection no longer exists the asynchronous evaluation method. The data frames are created by a message filter by assigning the detection and ground truth with approximate timestamps into one data frame. This means that there will be ground truth data that are not matched with any detection. Evaluating the \gls{ap} on the created data frames does not take this into consideration. Therefore, the \gls{ap} metric is calculated by multiplying the \gls{ap} on existing data frames and the overall processed frame rate. In this case, if a ground truth cannot be matched with any detection, the \gls{ap} of that particular data frame is considered as 0.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figures/setup/async_eval.pdf}
    \caption{Asynchronous evaluation}
    \label{fig:async_eval}
\end{figure}

% ------------------------------------
\section{Analysis}\label{sec:analysis}
% ------------------------------------

This section focuses on the analysis of the results from the experiments carried out in simulation and on a robotic system. First, the baseline offloading strategies are evaluated on the metrics defined in \cref{sec:evaluation_framework}. Then, the results of the dynamic offloading strategy are compared with the baseline strategies. 

% \subsection{Simulation experiment}

\subsection{Evaluation of baseline strategies}

When the network bandwidth is not constrained, the "edge only" strategy shows the best results in \gls{ap} of the object detection task, as shown in \cref{fig:robot_exp_ap}, while the "robot only" strategy has the worst results. As illustrated in \cref{fig:robot_exp_processed_frame_percentage_320}, the "edge only" strategy also processes the most frames in the experiments. The "robot only" strategy processes the least frames for about 50 percent of the total frames. However, the "edge only" strategy does not achieve the least \gls{rtt} due to a slight increase in the network delay of the edge perception. Naturally, the "edge only" strategy uses the least CPU and the least power consumption of the robot's onboard system. The "edge only" strategy also uses the most bandwidth of the robot. More than 270 Mbps bandwidth is used for perception offloading. 

% Figure for eth AP
\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]
    \input{figures/experiment/real_robot/eth/ap.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/ap.pgf}
    \caption{\gls{ap} of different offloading ratios using Ethernet in robot experiment}
    \label{fig:robot_exp_ap}
\end{figure}

The superiority of edge perception is mainly caused by two reasons. On one hand, the edge computer is using a more complex network, i.e., \gls{yolo}v5l, which provides more precise detection. On the other hand, the \gls{rtt} of edge perception is lower than the robot perception thanks to its low inference, as shown in \cref{fig:robot_exp_rtt_320}. In this case, the \gls{amr} is able to receive the detection from the edge faster than its onboard system. Moreover, the "edge only" strategy also processed the most frames in the experiments. This also makes the "edge only" strategy the safest among different strategies. With more detection frames, the \gls{amr} is able to adapt its behavior to the detection more quickly. Since the images are exclusively calculated on the edge computer, the "edge only" strategy uses less CPU and less energy. Moreover, the "edge only" strategy also processed the most frames in the experiments. This also makes the "edge only" strategy the safest among different strategies. With more detection frames, the \gls{amr} is able to adapt its behavior to the detection more quickly. However, since the network is not constrained, the \gls{amr} is free to use as much network bandwidth as it desires. For image feed with 25 \gls{fps} at a resolution of 848x640 pixels, around 244 Mbps bandwidth is used for offloading. Such bandwidth will be a huge amount for wireless connection. For comparison, IEEE 802.11ac (\gls{wifi} 5G) can support only up to 400 Mbps bandwidth with two 40 MHz channels \cite{IntelCorporation2021}. If the number of the \glspl{amr} scales, the network cannot provide such bandwidth. Therefore, the "edge only" strategy is not suitable for industrial usage.

% Figure for eth RTT 320
\begin{figure}
    \centering
    \input{figures/experiment/real_robot/eth/rtt_320.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/rtt_320.png}
    \caption{\gls{rtt} of different offloading ratios using Ethernet without bandwidth constraints in robot experiment}
    \label{fig:robot_exp_rtt_320}
\end{figure}

On the other hand, the "robot only" strategy delivers the worst results in the object detection task. This is mainly because the onboard resources are not enough for the \gls{amr} to process all the images. The robot perception module is strained when more images are calculated locally on the \gls{amr}'s onboard system. As shown in \cref{fig:robot_exp_rtt_160}, the robot perception has the highest \gls{rtt} with the "robot only" strategy. The \gls{rtt} of robot perception is broken down into different components. The inference time of the robot perception stays the same. However, the network delay is increased. This indicates that the robot perception module has too many images waiting for processing and the images start to queue, causing the network latency of robot perception to increase. The network latency of robot perception stabilizes around the offloading ratio of 0.6. This infers that the \gls{amr}'s onboard system, i.e., the \gls{nuc}, is only able to process less than 40 percent of the offloaded images, i.e., around 12 \gls{fps}. This corresponds to the 99.9 ms \gls{rtt} of the robot perception at the offloading ratio of 0.6. Naturally, the "robot only" strategy also uses the most \gls{cpu} and energy, since it computes all images onboard, as shown in \cref{fig:robot_exp_cpu_usage} and \cref{fig:robot_exp_energy}. Moreover, the "robot only" strategy still uses 0.4 MB/s network bandwidth because the state monitors need to send system state data from the edge computer to the robot's onboard system.

% Figure for processed frame percentage 320
\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/processed_frame_percentage_320.png}
    \input{figures/experiment/real_robot/eth/overall_processed_frame_percentage_320.pgf}
    \caption{Processed frame percentage of different offloading ratios using Ethernet without bandwidth constraints in robot experiment}
    \label{fig:robot_exp_processed_frame_percentage_320}
\end{figure}

As illustrated in \cref{fig:robot_exp_ap}, the increase of the \gls{ap} of the object detection task slows down between offloading ratios 0.5 and 0.8. This indicates that both the robot perception and the edge perception are working under their optimal workload. After this point, the edge perception becomes more dominant for the performance. The performance continues to improve because the edge is able to computer more frames and it causes the \gls{ap} to increase. 

% Figure for CPU usage
\begin{figure}
    \centering
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/cpu_usage.png}
    \input{figures/experiment/real_robot/eth/cpu_usage.pgf}
    \caption{CPU usage of different offloading ratios using Ethernet in robot experiment}
    \label{fig:robot_exp_cpu_usage}
\end{figure}


% Figure for CPU energy consumption
\begin{figure}
    \centering
    \input{figures/experiment/real_robot/eth/cpu_energy_consumption.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/cpu_energy_consumption.png}
    \caption{CPU energy consumption of different offloading ratios using Ethernet in robot experiment}
    \label{fig:robot_exp_energy}
\end{figure}

% Figure for network bandwidth
\begin{figure}
    \centering
    \input{figures/experiment/real_robot/eth/bandwidth.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/bandwidth.png}
    \caption{Network bandwidth usage of different offloading ratios using Ethernet in robot experiment}
    \label{fig:robot_exp_network_bandwidth}
\end{figure}

When the network is constrained, the "edge only" strategy shows the worst result in \gls{ap}, as shown in \cref{fig:robot_exp_ap}. The "edge only" strategy also processed the least frames among different offloading strategies, as shown in \cref{fig:robot_exp_processed_frame_percentage_160}. The "edge only" strategy still uses the least CPU and energy of the \gls{amr}'s onboard system. The bandwidth usage of the "edge only" strategy drops to 150 Mbps. The best performance occurs around the offloading ratio of 0.5. It also processes the most frames. Offloading 50 percent of the perception task also reduces CPU usage by 23 percent and reduces CPU power consumption by 16 percent, as shown in \cref{fig:robot_exp_cpu_usage} and \cref{fig:robot_exp_energy}. It also uses only 45 percent of the network bandwidth compared to the "edge only" strategy without bandwidth constraints, as shown in \cref{fig:robot_exp_network_bandwidth}. 

% Figure for processed frame percentage 160
\begin{figure}
    \centering
        \input{figures/experiment/real_robot/eth/overall_processed_frame_percentage_160.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/processed_frame_percentage_160.png}
    \caption{Processed frame percentage of different offloading ratios using Ethernet with 160 Mbps bandwidth constraint in robot experiment}
    \label{fig:robot_exp_processed_frame_percentage_160}
\end{figure}

Since the network is not capable of transmitting the amount of data the strategy needs, the messages start to queue, and the "best effort" reliable policy allows the middleware to drop many messages. This can be visualized and \cref{fig:robot_exp_processed_frame_percentage_160}. The processed frame percentage starts to drop drastically after the offloading ratio exceeds 0.5. This also corresponds to the network bandwidth in \cref{fig:robot_exp_network_bandwidth}. The network bandwidth is at its capacity after the offloading ratio exceeds 0.5. Furthermore, with the offloaded images hogging the network bandwidth, it is unlikely for the \glspl{amr} to transmit other data over the network, which can be essential for the safety of the \glspl{amr}. This makes the "edge only" strategy the worst strategy under constrained network conditions. 

% Figure for eth RTT 160
\begin{figure}
    \centering
    \input{figures/experiment/real_robot/eth/rtt_160.pgf}
    % \includegraphics[width=\linewidth]{figures/experiment/real_robot/eth/rtt_160.png}
    \caption{\gls{rtt} of different offloading ratios using Ethernet with 160 Mbps bandwidth constraint in robot experiment}
    \label{fig:robot_exp_rtt_160}
\end{figure}

On the other hand, the "robot only" strategy delivers similar results under the two network conditions. Since the "robot only" strategy computes the object detection locally on the onboard system, the performance should be independent of the network conditions. However, it is worth noticing that the network delay increases when the network is occupied with offloaded images. It is suspected that the \gls{netem} layer affects the \gls{ros} middleware and thus causes an increased latency in transmitting the data between the offloading module and the robot perception module. 

 At this offloading ratio of 0.5, the network bandwidth has not exceeded its limits and the robot perception is able to process the given workload. Therefore, the \glspl{rtt} of both perception modules are low and the \gls{amr} is able to get the detection in time. Moreover, offloading at this ratio also allows the robot to use fewer onboard resources. Therefore, under constrained network conditions, offloading a portion of the images is the best strategy. However, the offloading ratio is dependent on the available network bandwidth and the computation capabilities of the \gls{amr}'s onboard system.

% The decision-making strategy also delivers decent results for the object detection task under good network condition. As shown in \cref{fig:real_robot_experiment:eth_processed_frame_percentage_320}, the decision-making strategy offloads most images to the edge computer and only computes (TODO: add a number here) on the \gls{amr}'s onboard system. This decision can be justified by the low \gls{rtt} of the edge computer. (TODO: add analysis after getting the results). It it worth noticing that decision-making strategy calculates a small portion of the images locally on the onboard system, since the \gls{rtt} of edge perception can increase temporarily due to dynamic changes in the network. However, because the network condition is good, most of the images are offloaded to the edge computer. 

\subsection{Evaluation of dynamic offloading strategy}

 From the evaluation of the baseline strategies, the \gls{rtt} is an important metric to improve the performance of the perception task. Therefore, in the work of this thesis, the \gls{rtt} is used as a criterion for making offloading decisions for a dynamic offloading strategy, which is introduced in \cref{sec:offloading_decision}. The dynamic offloading strategy is evaluated under the same conditions as the evaluation carried out in the previous section. Three network conditions are used to evaluate the dynamic offloading strategy: Ethernet with 160 Mbps bandwidth constraint, Ethernet without bandwidth constraints, and Wi-Fi network. 

 The comparison conducted with an Ethernet connection with constrained bandwidth is shown in \cref{tab:dynamic_eth_160}. The dynamic offloading strategy is compared with the baseline strategies. In the baseline strategies, the "edge only" strategy, the "robot only" strategy, and the strategy that achieves the best \gls{ap} among different offloading ratios are selected to compare with the dynamic offloading strategy. The best results for different metrics are highlighted with bold text. When the network bandwidth is constrained, the dynamic offloading does not provide good results compared to the strategy that offloads with an offloading ratio of 0.5. However, it still manages to compute 45.47 percent of the frames, most of which are computed on the robot.

 % Table for dynamic offloading strategy with 160 Mbps bandwidth constraint
\begin{table}[htb]%
    \centering%
    \footnotesize
    \begin{tabular}{l|ccc|c}
        \toprule
        Strategies &                        Robot only &            Edge only &             Best among ratios &                dynamic offloading \\
        \midrule
        \gls{ap} &                          $24.30\%\pm1.54\%$ &    $0.16\%\pm0.01\%$ &     \textbf{$54.44\%\pm0.70\%$} &        $23.21\%\pm8.67\%$\\
        Robot perception \gls{rtt} / ms &        $729.83\pm 70.02$ &     / &                     \textbf{$297.63\pm25.35$} &          $731.50\pm130.64$\\
        Edge perception \gls{rtt} / ms &          / &                    $479\pm9.36$ &         \textbf{$322.59\pm0.72$} &            $1195.19\pm99.52$\\
        Processed frames percentage &       $51.37\%\pm0.93\%$ &    $0.22\%\pm0$ &          \textbf{$78.66\%\pm0.75\%$} &        $45.47\%\pm15.74\%$\\
        \midrule
        CPU usage &                         $43.49\% \pm 0.67\%$ &  \textbf{$0.73\% \pm 1.86\%$} &    $36.68\% \pm 0.88\%$ &     $35.32\% \pm 13.64\%$ \\
        Power consumption / mJ/s &      $14.54 \pm 0.39$ &      \textbf{$2.09 \pm 0.11 $} &       $11.32 \pm 0.12$ &         $11.91 \pm 3.69$\\
        Bandwidth usage / Mbps &            \textbf{$2.73 \pm 0.08$} &       $151.67 \pm 0.98$ &      $97.64 \pm 0.91$ &         $38.18 \pm 42.60$ \\
        
        \bottomrule
    \end{tabular}
    \caption{Metrics of dynamic offloading strategy compared with baseline strategies using Ethernet with 160 Mbps bandwidth constraint}
    \label{tab:dynamic_eth_160}%
\end{table}

 The reason for the poor results of the dynamic offloading strategy under constrained network bandwidth is that even though the \gls{amr} can hardly get any detection back from the edge perception node, there are still a small number of detection received by the edge perception. The inference time and the network delay of them are used to compute the \gls{rtt} of the edge perception. However, the values of \gls{rtt} are relatively low thanks to the "best effort" strategy used for offloading. Therefore, the offloading module prefers the edge perception over the robot perception. This causes the results of the dynamic offloading strategy to deteriorate under constrained network bandwidth. 

The comparison conducted with an unconstrained Ethernet connection is shown in \cref{tab:dynamic_eth_320}. Since the "edge only" strategy outperform all strategies with different offloading ratios in terms of \gls{ap}, only the "edge only" strategy is included in the table. There, the dynamic offloading strategy offers similar metrics to the "edge only" strategy. The dynamic offloading strategy achieves the best \gls{ap} among all offloading strategies and achieves the lowest \gls{rtt} of robot perception compared to the "robot only" and the "edge only" strategy. However, it is worth noticing that the dynamic offloading strategy does not offload all frames to the edge computer. It still keeps some frames computed locally with the robot's onboard system. 

 % Table for dynamic offloading strategy without bandwidth constraints
\begin{table}[htb]%
    \centering%
    \footnotesize
    \begin{tabular}{l|cc|c}
        \toprule
        Strategies &                        Robot only &            Edge only &              dynamic offloading  \\
        \midrule
        \gls{ap} &                          $22.57\%\pm1.89\%$ &    $76.53\%\pm0.52\%$ &     \textbf{$77.02\%\pm0.46\%$}  \\
        Robot perception \gls{rtt}\\ / ms &   $811.75\pm 107.29$ &     / &                      \textbf{$731.50\pm145.84$}   \\
        Edge perception \gls{rtt} / ms &    / &                     \textbf{$168.82\pm8.96$} &        $177.27\pm13.04$    \\
        Processed frames percentage &       $50.87\%\pm1.57\%$ &    \textbf{$99.85\%\pm0.45\%$} &     $99.84\%\pm0.15\%$  \\
        \midrule
        CPU usage &                         $44.45\% \pm 1.10\%$ &  \textbf{$2.15\% \pm 0.93\%$} &    $3.36\% \pm 1.22\%$ \\
        Power consumption / mJ/s &      $13.98 \pm 0.39$ &      \textbf{$0.98 \pm 0.05$} &        $1.20 \pm 0.07$     \\
        Bandwidth usage / Mbps &            \textbf{$2.18 \pm 0.04$} &       $271.46 \pm 1.73$ &      $272.84 \pm 1.85$   \\
        \bottomrule
    \end{tabular}
    \caption{Metrics of dynamic offloading strategy compared with baseline strategies using Ethernet without bandwidth constraints}
    \label{tab:dynamic_eth_320}%
\end{table}

Finally, the dynamic offloading strategy is evaluated on a \gls{wifi} network. The dynamic offloading strategy provides decent results under the \gls{wifi} network. It manages to achieve 45.81 percent of \gls{ap} and processes over 80 percent of the overall frames. Compared to the strategy that offloads with a ratio of 0.5, the dynamic offloading strategy uses less onboard CPU and less onboard power. However, it uses more network bandwidth on average and the \glspl{rtt} of the robot perception and the edge perception is comparably higher. 

 % Table for dynamic offloading strategy without bandwidth constraints
\begin{table}[htb]%
    \centering%
    \footnotesize
    \begin{tabular}{l|ccc|c}
        \toprule
        Strategies &                        Robot only &            Edge only &                     Best among ratios &           dynamic offloading      \\
        \midrule
        \gls{ap} &                          $25.17\%\pm2.25\%$ &    $32.20\%\pm14.73\%$ &           \textbf{$61.60\%\pm0.78\%$} &       $45.81\%\pm22.34\%$      \\
        Robot perception \gls{rtt} / ms &   $653.70\pm 130.30$ &    / &                             \textbf{$177.24\pm21.56$} &         $992.06\pm226.89$       \\
        Edge perception \gls{rtt} / ms &    / &                     $606.02\pm226.21$ &             \textbf{$217.26\pm7.56$} &          $607.47\pm410.55$      \\
        Processed frames percentage &       $49.01\%\pm1.31\%$ &    $71.60\%\pm26.38\%$ &           \textbf{$84.92\%\pm0.82\%$} &       $80.31\%\pm34.85\%$     \\
        \midrule
        CPU usage &                         $43.55\% \pm 1.29\%$ &  \textbf{$3.42\% \pm 2.29\%$} &  $39.49\% \pm 1.34\%$ &              $8.86\% \pm 7.70\%$    \\
        Power consumption / mJ/s &      $13.37 \pm 0.42$ &      \textbf{$0.58 \pm 0.25$} &      $11.16 \pm 0.24$ &                  $2.03 \pm 2.44$        \\
        Bandwidth usage / Mbps &            \textbf{$1.86 \pm 0.11$} &       $112.21 \pm 72.77$ &   $124.12 \pm 26.70$ &                $203.79 \pm 93.75$       \\
        \bottomrule
    \end{tabular}
    \caption{Metrics of dynamic offloading strategy compared with baseline strategies using Wi-Fi}
    \label{tab:dynamic_eth_wifi}%
\end{table}

It is worth noticing that the metrics have high standard deviation under the \gls{wifi} network. The reason is that the network condition is inconsistent among different experiment runs. However, such fluctuation is intended to investigate the performance of the dynamic offloading strategy. The difference between the dynamic offloading strategy and the ratio strategy is caused by two factors. First, the ratio offloading strategy achieves lower \gls{rtt} of the edge and the robot perception. Second, the ratio strategy also processes more frames than the dynamic offloading strategy and thus achieves better results in \gls{ap}. Moreover, with the low standard deviation of the ratio strategy, the better results could be caused by a transient maximum of the network bandwidth during the experiment. 

% Under constrained network condition, the decision-making strategy also managed to deliver decent results. The increased \gls{rtt} time forces the \gls{amr} to compute most of the images locally. However, the decision-making strategy still offloads a portion of the images to the onboard system, because the onboard system is not able to process all images and \gls{rtt} of the robot perception will increase drastically if the images start to queue. These results show that the decision-making strategy is able adapt to the dynamic changes of the network by simply using the \gls{rtt} as a criterion. Furthermore, the \gls{amr} uses more \gls{cpu} and energy with the decision-making strategy under constrained network condition compared to unconstrained network condition, since the \glspl{amr} have to do more computation on the board system and thus consume more onboard resources, as shown in \cref{fig:real_robot_experiment:eth_cpu_energy_consumption} and \cref{fig:real_robot_experiment:eth_cpu_percentage}.
